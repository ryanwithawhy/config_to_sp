#,subsection,name,definition,display_definition,what to do,type,default,valid_values,importance,
49,Consumer configuration,max.poll.interval.ms,"The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 300000 milliseconds (5 minutes).","The maximum delay between subsequent consume requests to Kafka. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 1000 milliseconds (1 second).",ALLOW,long,1000,"[60000,…,1800000] for non-dedicated clusters and [60000,…] for dedicated clusters",low,
3,Input messages,input.data.format,"Sets the input Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON, STRING or BSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF.",,ALLOW default,string,JSON,,high,
4,Input messages,input.key.format,"Sets the input Kafka record key format. Valid entries are AVRO, BYTES, JSON, JSON_SR, PROTOBUF, or STRING. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",,ALLOW default,string,STRING,"AVRO, BYTES, JSON, JSON_SR, PROTOBUF, STRING",high,
5,Input messages,cdc.handler,"The class name of the CDC handler to use for processing. You can capture CDC events with the MongoDB Kafka sink connector and perform corresponding insert, update, and delete operations to a destination MongoDB cluster.",,ALLOW default,string,None,"None , MongoDbChangeStreamHandler , DebeziumMongoDbHandler , DebeziumMySqlHandler , DebeziumPostgresHandler , or QlikRdbmsHandler",low,
6,Writes,delete.on.null.values,Whether or not the connector should try to delete documents based on key when value is null.,,ALLOW default,boolean,FALSE,,low,
11,Writes,write.strategy,The class that specifies the WriteModel to use for bulk writes.,,ALLOW default,string,DefaultWriteModelStrategy,"DefaultWriteModelStrategy , ReplaceOneDefaultStrategy , InsertOneDefaultStrategy , ReplaceOneBusinessKeyStrategy , DeleteOneDefaultStrategy , UpdateOneTimestampsStrategy , UpdateOneBusinessKeyTimestampStrategy , or UpdateOneDefaultStrategy . If not used, this property defaults to DefaultWriteModelStrategy . For time-series collections, the DefaultWriteModelStrategy will internally default to InsertOneDefaultStrategy . For normal collections, it defaults to ReplaceOneDefaultStrategy .",low,
23,ID strategies,doc.id.strategy,The IdStrategy class name to use for generating a unique document id (_id).,,ALLOW default,string,BsonOidStrategy,"BsonOidStrategy , KafkaMetaDataStrategy , FullKeyStrategy , PartialKeyStrategy , PartialValueStrategy , ProvidedInKeyStrategy , ProvidedInValueStrategy , or UuidStrategy . To delete the document when the value is null, you must set the strategy to FullKeyStrategy , PartialKeyStrategy , or ProvidedInKeyStrategy",low,
24,ID strategies,doc.id.strategy.overwrite.existing,Whether the connector should overwrite existing values in the _id field when the strategy defined in doc.id.strategy is applied.,,ALLOW default,boolean,FALSE,,low,
25,ID strategies,document.id.strategy.uuid.format,The bson output format when using the UuidStrategy. Can be either String or Binary.,,ALLOW default,string,string,,low,
52,Additional Configs,consumer.override.auto.offset.reset,"Defines the behavior of the consumer when there is no committed position (which occurs when the group is first initialized) or when an offset is out of range. You can choose either to reset the position to the ""earliest"" offset or the ""latest"" offset (the default). You can also select ""none"" if you would rather set the initial offset yourself and you are willing to handle out of range errors manually. More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#auto-offset-reset","Defines the behavior of the consumer when there is no committed position (which occurs when the group is first initialized) or when an offset is out of range. You can choose either to reset the position to the ""earliest"" offset or the ""latest"" offset (the default).","ALLOW earliest, latest",string,,latest,low,
17,Which topics do you want to get data from?,errors.deadletterqueue.topic.name,"The name of the topic to be used as the dead letter queue (DLQ) for messages that result in an error when processed by this sink connector, or its transformations or converters. Defaults to 'dlq-${connector}' if not set. The DLQ topic will be created automatically if it does not exist. You can provide ${connector} in the value to use it as a placeholder for the logical cluster ID.",,DISALLOW,string,dlq-${connector},,low,
26,ID strategies,key.projection.type,For use with the PartialKeyStrategy allows custom key fields to be projected for the ID strategy. Use either AllowList or BlockList.,,DISALLOW,string,none,,low,
27,ID strategies,key.projection.list,For use with the PartialKeyStrategy allows custom key fields to be projected for the ID strategy. A comma-separated list of field names for key projection.,,DISALLOW,string,,,low,
28,ID strategies,value.projection.type,For use with the PartialValueStrategy allows custom value fields to be projected for the ID strategy. Use either AllowList or BlockList.,,DISALLOW,string,none,,low,
29,ID strategies,value.projection.list,For use with the PartialValueStrategy allows custom value fields to be projected for the ID strategy. A comma-separated list of field names for value projection.,,DISALLOW,string,,,low,
30,Namespace mapping,namespace.mapper.class,The class that determines the namespace to write the sink data to. By default this will be based on the 'database' configuration and either the topic name or the 'collection' configuration.,,DISALLOW,string,DefaultNamespaceMapper,,low,
31,Namespace mapping,namespace.mapper.key.database.field,The key field to use as the destination database name.,,DISALLOW,string,,,low,
32,Namespace mapping,namespace.mapper.key.collection.field,The key field to use as the destination collection name.,,DISALLOW,string,,,low,
33,Namespace mapping,namespace.mapper.value.database.field,The value field to use as the destination database name.,,DISALLOW,string,,,low,
34,Namespace mapping,namespace.mapper.value.collection.field,The value field to use as the destination collection name.,,DISALLOW,string,,,low,
35,Namespace mapping,namespace.mapper.error.if.invalid,Whether to throw an error if the mapped field is missing or invalid. Defaults to false.,,DISALLOW,boolean,FALSE,,low,
41,Time Series configuration,timeseries.timefield,The name of the top-level field which contains the date in each time series document. Setting this config will create a time series collection where each document will have a BSON date as the value for the timefield.,,DISALLOW,string,,,low,
42,Time Series configuration,timeseries.timefield.auto.convert,"Whether to convert the data in the field into a BSON Date format. Supported formats include integer, long, and string.",,DISALLOW,boolean,FALSE,,low,
43,Time Series configuration,timeseries.timefield.auto.convert.date.format,"The string pattern to convert the source data from. The setting expects the string representation to contain both date and time information and uses the Java DateTimeFormatter.ofPattern(pattern, locale) API for the conversion. If the string only contains date information, then the time since epoch is from the start of that day. If a string representation does not contain time-zone offset, then the setting interprets the extracted date and time as UTC.",,DISALLOW,string,yyyy-MM-dd[['T'][ ]][HH:mm:ss[[.][SSSSSS][SSS]][ ]VV[ ]'['VV']'][HH:mm:ss[[.][SSSSSS][SSS]][ ]X][HH:mm:ss[[.][SSSSSS][SSS]]],,low,
44,Time Series configuration,timeseries.timefield.auto.convert.locale.language.tag,The DateTimeFormatter locale language tag to use with the date pattern.,,DISALLOW,string,en,,low,
45,Time Series configuration,timeseries.metafield,The name of the top-level field that contains metadata in each time series document. This field groups related data. It can be of any type except array.,,DISALLOW,string,,,low,
46,Time Series configuration,timeseries.expire.after.seconds,The amount of seconds the data remains in MongoDB before MongoDB deletes it. Omitting this field means data will not be deleted automatically.,,DISALLOW,int,0,"[0,…]",low,
47,Time Series configuration,ts.granularity,The expected interval between subsequent measurements for a time-series. Set this to None or leave it empty if the data is not time-series,,DISALLOW,string,None,"None , seconds , minutes , or hours",low,
53,Additional Configs,consumer.override.isolation.level,"Controls how to read messages written transactionally. If set to read_committed, consumer.poll() will only return transactional messages which have been committed. If set to read_uncommitted (the default), consumer.poll() will return all messages, even transactional messages which have been aborted. Non-transactional messages will be returned unconditionally in either mode. More details: https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#isolation-level",,DISALLOW,string,,,low,
86,CSFLE,csfle.enabled,Flag to indicate whether the connector honors CSFLE rules.,,DISALLOW,boolean,,,low,
87,CSFLE,sr.service.account.id,A Service Account to access the Schema Registry and associated encryption rules or keys with that schema.,,DISALLOW,string,,,low,
88,CSFLE,csfle.onFailure,"Configures the connector behavior ( ERROR or NONE ) on data decryption failure. If set to ERROR , the connector fails and writes the encrypted data in the DLQ. If set to NONE , the connector writes the encrypted data in the target system without decryption.",,DISALLOW,string,,,low,
7,Writes,max.batch.size,The maximum number of sink records to possibly batch together for processing.,,IGNORE,int,0,"[0,…]",low,
8,Writes,bulk.write.ordered,Whether the batches controlled by 'max.batch.size' must be written via ordered bulk writes.,,IGNORE,boolean,TRUE,,low,
9,Writes,rate.limiting.timeout,How long in ms processing should wait before continuing after triggering a rate limit.,,IGNORE,int,0,,low,
10,Writes,rate.limiting.every.n,The number of processed batches that will trigger rate limiting. The default value of 0 sets no rate limiting.,,IGNORE,int,0,,low,
39,Connection details,max.num.retries,How many retries should be attempted on write errors.,,IGNORE,int,3,"[0,…]",low,
40,Connection details,retries.defer.timeout,How long a retry should get deferred.,,IGNORE,int,5000,"[0,…]",low,
50,Consumer configuration,max.poll.records,"The maximum number of records to consume from Kafka in a single request. This configuration property may be used to improve the performance of the connector, if the connector cannot send records to the sink system. Defaults to 500 records.",,IGNORE,long,500,"[1,…,500] for non-dedicated clusters and [1,…] for dedicated clusters",low,
16,Which topics do you want to get data from?,topics,Identifies the topic name or a comma-separated list of topic names.,Identifies the topic name or a comma-separated list of topic names.,REQUIRE,list,,,high,
13,Database details,collection,Single MongoDB Atlas collection to write to.,Single MongoDB Atlas collection to write to.,REQUIRE,string,,,medium,1