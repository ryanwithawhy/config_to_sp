#,subsection,name,definition,type,what_do_do,default,valid_values,importance
7,How do you want to name your topic(s)?,topic.prefix,Prefix to prepend to table names to generate the name of the Apache Kafka® topic to publish data to.,string,REQUIRE,,,high
8,How do you want to name your topic(s)?,topic.namespace.map,"JSON object that maps change stream document namespaces to topics. Any prefix configuration will still apply. In case multiple collections with records having varying schema are mapped to single topic with AVRO, JSON_SR, and PROTOBUF, then multiple schemas will be registered under single subject name. If these schemas are not backward compatible to each other, the connector will fail until you change the schema compatibility in Confluent Cloud Schema Registry.",string,DISALLOW,,,low
14,Connection details,poll.await.time.ms,The amount of time to wait before checking for new results on the change stream.,int,IGNORE,5000 (5 seconds),"[1,…]",low
15,Connection details,poll.max.batch.size,Maximum number of change stream documents to include in a single batch when polling for new data. This setting can be used to limit the amount of data buffered internally in the connector.,int,IGNORE,100,"[1,…,1000]",low
16,Connection details,pipeline,"An array of JSON objects describing the pipeline operations to filter or modify the change events output. For example, [{""$match"": {""ns.coll"": {""$regex"": /^(collection1|collection2)$/}}}] will set your source connector to listen to the ""collection1"" and ""collection2"" collections only.",string,ALLOW,[],,medium
17,Connection details,startup.mode,"Specifies how the connector should start up when there is no source offset available. If set to 'latest', the connector ignores all existing source data. If set to 'timestamp', the connector actuates startup.mode.timestamp.* properties. If no properties are configured, timestamp is equivalent to latest. If startup.mode=copy_existing, the connector copies all existing source data to Change Stream events.",string,"ALLOW latest, copy_existing",,,high
18,Connection details,startup.mode.copy.existing.namespace.regex,"Regular expression that matches the namespaces (databaseName.collectionName) from which to copy data. For example, stats.page.* matches all collections that starts with ""page"" in ""stats"" database.",string,DISALLOW,,,medium
19,Connection details,startup.mode.copy.existing.pipeline,An array of JSON objects describing the pipeline operations to run when copying existing data. It will only be applied for existing documents which are being copied.,string,DISALLOW,,,medium
20,Connection details,startup.mode.timestamp.start.at.operation.time,Actuated only if startup.mode=timestamp. Specifies the starting point for the change stream.,string,DISALLOW,,,medium
21,Connection details,batch.size,The number of documents to return in a batch.,int,IGNORE,0,"[…,50]",low
22,Connection details,output.schema.key,The Avro schema definition for the key value of the SourceRecord.,string,IGNORE,"{ ""type"": ""record"", ""name"": ""keySchema"", ""fields"": [{ ""name"": ""_id"", ""type"": ""string""}]}",A string at most 100000 characters long,medium
23,Connection details,output.schema.value,The Avro schema definition for the value of the SourceRecord.,string,IGNORE,"{""name"": ""ChangeStream"", ""type"": ""record"", ""fields"": [{""name"": ""_id"", ""type"": ""string""}, {""name"": ""operationType"", ""type"": [""string"", ""null""]}, {""name"": ""fullDocumentBeforeChange"", ""type"": [""string"", ""null""]}, {""name"": ""fullDocument"", ""type"": [""string"", ""null""]}, {""name"": ""ns"", ""type"": [{""name"": ""ns"", ""type"": ""record"", ""fields"": [{""name"": ""db"", ""type"": ""string""}, {""name"": ""coll"", ""type"": [""string"", ""null""]}]}, ""null""]}, {""name"": ""to"", ""type"": [{""name"": ""to"", ""type"": ""record"", ""fields"": [{""name"": ""db"", ""type"": ""string""}, {""name"": ""coll"", ""type"": [""string"", ""null""]}]}, ""null""]}, {""name"": ""documentKey"", ""type"": [""string"", ""null""]}, {""name"": ""updateDescription"", ""type"": [{""name"": ""updateDescription"", ""type"": ""record"", ""fields"": [{""name"": ""updatedFields"", ""type"": [""string"", ""null""]}, {""name"": ""removedFields"", ""type"": [{""type"": ""array"", ""items"": ""string""}, ""null""]}]}, ""null""]}, {""name"": ""clusterTime"", ""type"": [""string"", ""null""]}, {""name"": ""txnNumber"", ""type"": [""long"", ""null""]}, {""name"": ""lsid"", ""type"": [{""name"": ""lsid"", ""type"": ""record"", ""fields"": [{""name"": ""id"", ""type"": ""string""}, {""name"": ""uid"", ""type"": ""string""}]}, ""null""]}]}",A string at most 100000 characters long,medium
24,Producer configuration,linger.ms,Artificial delay for records to be sent together.,long,IGNORE,0,"[0,…,20000]",medium
25,Producer configuration,producer.batch.size,Record batch size in bytes.,int,IGNORE,16384,"[0,…,491520]",medium
26,Output messages,output.data.format,"Sets the output Kafka record value format. Valid entries are AVRO, JSON_SR, PROTOBUF, JSON, STRING or BSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",string,ALLOW JSON,STRING,"AVRO, JSON_SR, PROTOBUF, JSON, STRING, BSON",high
27,Output messages,output.key.format,"Sets the output Kafka record key format. Valid entries are AVRO, JSON_SR, PROTOBUF, STRING or JSON. Note that you need to have Confluent Cloud Schema Registry configured if using a schema-based message format like AVRO, JSON_SR, and PROTOBUF",string,ALLOW STRING,STRING,"AVRO, JSON, JSON_SR, PROTOBUF, STRING",high
28,Output messages,publish.full.document.only,Only publish the changed document instead of the full change stream document. Sets the change.stream.full.document=updateLookup automatically so updated documents will be included.,boolean,"ALLOW TRUE, FALSE",FALSE,,high
29,Output messages,publish.full.document.only.tombstone.on.delete,Return the tombstone events when documents are deleted. Tombstone events contain the keys of deleted documents with null values. This setting applies only when publish.full.document.only is true,boolean,DISALLOW,FALSE,,high
30,Output messages,change.stream.full.document,"Determines what to return for update operations when using a Change Stream. When set to 'updateLookup' setting returns the differences between the original document and updated document as well as a copy of the entire updated document at a point in time after the update. The 'whenAvailable' setting returns the updated document, if available. The 'required' setting returns the updated document and raises an error if it is not available.",string,"ALLOW updateLookup, required, whenAvailable",updateLookup,,high
31,Output messages,change.stream.full.document.before.change,"Configures the document pre-image your change stream returns on update operations. When set to 'whenAvailable' setting returns the document pre-image if it's available, before it was replaced, updated, or deleted. When set to 'required' setting returns the document pre-image and raises an error if it is not available.",string,"ALLOW whenAvailable, required, off",off,,high
32,Output messages,output.json.format,"The output format of json strings can be configured to be either: DefaultJson: The legacy strict json formatter. ExtendedJson: The fully type safe extended json formatter. SimplifiedJson: Simplified Json, with ObjectId, Decimals, Dates and Binary values represented as strings. Users can provide their own implementation of the com.mongodb.kafka.connect.source.json.formatter.",string,"ALLOW DefaultJson, ExtendedJson, SimplifiedJson",DefaultJson,,high
33,Output messages,topic.separator,"Separator to use when joining prefix, database, collection, and suffix values. This generates the name of the Kafka topic to publish data to. Used by the 'DefaultTopicMapper'.",string,ALLOW,.,,low
34,Output messages,topic.suffix,Suffix to append to database and collection names to generate the name of the Kafka topic to publish data to.,string,ALLOW,,,low
35,Output messages,output.schema.infer.value,"Whether the connector should infer the schema for the value document of the Source Record. Since the connector processes each document in isolation, the connector may generate many schemas. The connector only reads this setting when you set your 'Output Kafka record value format' setting to AVRO, JSON, JSON_SR and PROTOBUF.",boolean,DISALLOW,TRUE,,low
36,Error handling,heartbeat.interval.ms,"The number of milliseconds the connector waits between sending heartbeat messages. The connector sends heartbeat messages when source records are not published in the specified interval. This mechanism improves resumability of the connector for low volume namespaces. When using SMTs, use predicates to prevent SMTs from processing the heartbeat messages. See connector documentation for more details.",int,IGNORE,0,,medium
37,Error handling,heartbeat.topic.name,"The name of the topic on which the connector should publish heartbeat messages. You must provide a positive value in the ""heartbeat.interval.ms"" setting to enable this feature.",string,IGNORE,__mongodb_heartbeats,,medium
38,Error handling,offset.partition.name,"The custom offset partition name to use. You can use this option to instruct the connector to start a new change stream when an existing offset contains an invalid resume token. If you leave this setting blank, the connector uses the default partition name based on the connection details.",string,DISALLOW,,,medium
39,Error handling,remove.field.on.schema.mismatch,"If true, remove fields from the document that are not present in the schema. Otherwise, throw an error or send the documents to the DLQ depending on the value of errors.tolerance being set to ALL or NONE respectively.",boolean,DISALLOW,TRUE,,medium
41,Error handling,mongo.errors.deadletterqueue.topic.name,"Whether to output conversion errors to the dead letter queue. Stops poison messages when using schemas, any message will be outputted as extended json on the specified topic. By default messages are not outputted to the dead letter queue. Also requires errors.tolerance=all.",string,DISALLOW,,,medium
47,Additional Configs,producer.override.compression.type,The compression type for all data generated by the producer.,string,"ALLOW none, gzip, snappy, lz4, zstd",none,,low
48,Additional Configs,producer.override.linger.ms,The producer groups together any records that arrive in between request transmissions into a single batched request. More details can be found in the documentation: https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#linger-ms.,long,IGNORE,,"[100,…,1000]",low
13,Database details,collection,"Single MongoDB Atlas collection to watch. If not set, all collections in the specified database are watched.",string,ALLOW,,,medium