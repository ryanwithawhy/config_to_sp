#,subsection,name,original_definition,display_definition,type,what_do_do,default,valid_values,importance
1,How should we connect to your data?,name,Sets a name for your connector.,Sets a name for your stream processor.,string,REQUIRE,,A string at most 64 characters long,high
2,Kafka Cluster credentials,kafka.auth.mode,Kafka Authentication mode. It can be one of KAFKA_API_KEY or SERVICE_ACCOUNT. It defaults to KAFKA_API_KEY mode.,,string,ALLOW default,KAFKA_API_KEY,"KAFKA_API_KEY, SERVICE_ACCOUNT",high
3,Kafka Cluster credentials,kafka.api.key,Kafka API Key. Required when kafka.auth.mode==KAFKA_API_KEY.,Kafka API Key.  Required to connect to your Kafka cluster.,password,REQUIRE,,,high
4,Kafka Cluster credentials,kafka.service.account.id,The Service Account that will be used to generate the API keys to communicate with Kafka Cluster.,,string,DISALLOW,,,high
5,Kafka Cluster credentials,kafka.api.secret,Secret associated with Kafka API key. Required when kafka.auth.mode==KAFKA_API_KEY.,Secret associated with Kafka API key.,password,REQUIRE,,,high
6,Schema Config,schema.context.name,"Add a schema context name. A schema context represents an independent scope in Schema Registry. It is a separate sub-schema tied to topics in different Kafka clusters that share the same Schema Registry instance. If not used, the connector uses the default schema configured for Schema Registry in your Confluent Cloud environment.",,string,DISALLOW,default,,medium
9,How should we connect to your MongoDB Atlas database?,connection.host,MongoDB Atlas connection host (e.g. confluent-test.mycluster.mongodb.net).,,string,IGNORE,,,high
10,How should we connect to your MongoDB Atlas database?,connection.user,MongoDB Atlas connection user.,MongoDB Atlas connection user.,string,REQUIRE,,,high
11,How should we connect to your MongoDB Atlas database?,connection.password,MongoDB Atlas connection password.,MongoDB Atlas connection password.,password,REQUIRE,,,high
12,How should we connect to your MongoDB Atlas database?,database,"MongoDB Atlas database name. If not set, all databases in the cluster are watched.","MongoDB Atlas database name. If not set, all databases in the cluster are watched.",string,REQUIRE,,,high
13,Database details,collection,"Single MongoDB Atlas collection to watch. If not set, all collections in the specified database are watched.","Single MongoDB Atlas collection to watch. If not set, all collections in the specified database are watched.",string,ALLOW,,,medium
40,Error handling,mongo.errors.tolerance,Use this property if you would like to configure the connector's error handling behavior differently from the Connect framework's.,,string,IGNORE,NONE,,medium
42,Server API,server.api.version,The server API version to use. Disabled by default.,,string,IGNORE,,,low
43,Server API,server.api.deprecation.errors,Sets whether the connector requires use of deprecated server APIs to be reported as errors.,,boolean,IGNORE,FALSE,,low
44,Server API,server.api.strict,Sets whether the application requires strict server API version enforcement.,,boolean,IGNORE,FALSE,,low
45,Number of tasks for this connector,tasks.max,Maximum number of tasks for the connector.,,int,IGNORE,,"[1,…,1]",high
46,Additional Configs,header.converter,The converter class for the headers. This is used to serialize and deserialize the headers of the messages.,,string,DISALLOW,,,low
49,Additional Configs,value.converter.allow.optional.map.keys,Allow optional string map key when converting from Connect Schema to Avro Schema. Applicable for Avro Converters.,,boolean,IGNORE,,,low
50,Additional Configs,value.converter.auto.register.schemas,Specify if the Serializer should attempt to register the Schema.,,boolean,IGNORE,,,low
51,Additional Configs,value.converter.connect.meta.data,Allow the Connect converter to add its metadata to the output schema. Applicable for Avro Converters.,,boolean,IGNORE,,,low
52,Additional Configs,value.converter.enhanced.avro.schema.support,Enable enhanced schema support to preserve package information and Enums. Applicable for Avro Converters.,,boolean,IGNORE,,,low
53,Additional Configs,value.converter.enhanced.protobuf.schema.support,Enable enhanced schema support to preserve package information. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
54,Additional Configs,value.converter.flatten.unions,Whether to flatten unions (oneofs). Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
55,Additional Configs,value.converter.generate.index.for.unions,Whether to generate an index suffix for unions. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
56,Additional Configs,value.converter.generate.struct.for.nulls,Whether to generate a struct variable for null values. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
57,Additional Configs,value.converter.int.for.enums,Whether to represent enums as integers. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
58,Additional Configs,value.converter.latest.compatibility.strict,Verify latest subject version is backward compatible when use.latest.version is true.,,boolean,IGNORE,,,low
59,Additional Configs,value.converter.object.additional.properties,Whether to allow additional properties for object schemas. Applicable for JSON_SR Converters.,,boolean,IGNORE,,,low
60,Additional Configs,value.converter.optional.for.nullables,Whether nullable fields should be specified with an optional label. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
61,Additional Configs,value.converter.optional.for.proto2,Whether proto2 optionals are supported. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
62,Additional Configs,value.converter.scrub.invalid.names,Whether to scrub invalid names by replacing invalid characters with valid characters. Applicable for Avro and Protobuf Converters.,,boolean,IGNORE,,,low
63,Additional Configs,value.converter.use.latest.version,Use latest version of schema in subject for serialization when auto.register.schemas is false.,,boolean,DISALLOW,,,low
64,Additional Configs,value.converter.use.optional.for.nonrequired,Whether to set non-required properties to be optional. Applicable for JSON_SR Converters.,,boolean,IGNORE,,,low
65,Additional Configs,value.converter.wrapper.for.nullables,Whether nullable fields should use primitive wrapper messages. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
66,Additional Configs,value.converter.wrapper.for.raw.primitives,Whether a wrapper message should be interpreted as a raw primitive at root level. Applicable for Protobuf Converters.,,boolean,IGNORE,,,low
67,Additional Configs,errors.tolerance,"Use this property if you would like to configure the connector's error handling behavior. WARNING: This property should be used with CAUTION for SOURCE CONNECTORS as it may lead to dataloss. If you set this property to 'all', the connector will not fail on errant records, but will instead log them (and send to DLQ for Sink Connectors) and continue processing. If you set this property to 'none', the connector task will fail on errant records.","Use this property if you would like to configure the connector's error handling behavior.  If you set this property to 'all', the connector will not fail on errant records, but will instead store them and their error messages in your Atlas cluster in a collection named dlq.<your-stream-processor-name>.  If you set this property to 'ignore', the connector will discard errant records and keep processing new ones.",string,"ALLOW all, ignore",all,,low
68,Additional Configs,key.converter.key.subject.name.strategy,How to construct the subject name for key schema registration.,,string,DISALLOW,TopicNameStrategy,,low
69,Additional Configs,key.converter.replace.null.with.default,"Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Key Converter.",,boolean,DISALLOW,TRUE,,low
70,Additional Configs,key.converter.schemas.enable,"Include schemas within each of the serialized keys. Input message keys must contain schema and payload fields and may not contain additional fields. For plain JSON data, set this to false. Applicable for JSON Key Converter.",,boolean,DISALLOW,FALSE,,low
71,Additional Configs,value.converter.decimal.format,Specify the JSON/JSON_SR serialization format for Connect DECIMAL logical type values with two allowed literals: BASE64 to serialize DECIMAL logical types as base64 encoded binary data and NUMERIC to serialize Connect DECIMAL logical type values in JSON/JSON_SR as a number representing the decimal value.,,string,IGNORE,BASE64,,low
72,Additional Configs,value.converter.flatten.singleton.unions,Whether to flatten singleton unions. Applicable for Avro and JSON_SR Converters.,,boolean,IGNORE,FALSE,,low
73,Additional Configs,value.converter.ignore.default.for.nullables,"When set to true, this property ensures that the corresponding record in Kafka is NULL, instead of showing the default column value. Applicable for AVRO,PROTOBUF and JSON_SR Converters.",,boolean,IGNORE,FALSE,,low
74,Additional Configs,value.converter.reference.subject.name.strategy,Set the subject reference name strategy for value. Valid entries are DefaultReferenceSubjectNameStrategy or QualifiedReferenceSubjectNameStrategy. Note that the subject reference name strategy can be selected only for PROTOBUF format with the default strategy being DefaultReferenceSubjectNameStrategy.,,string,IGNORE,DefaultReferenceSubjectNameStrategy,"DefaultReferenceSubjectNameStrategy, QualifiedReferenceSubjectNameStrategy",low
75,Additional Configs,value.converter.replace.null.with.default,"Whether to replace fields that have a default value and that are null to the default value. When set to true, the default value is used, otherwise null is used. Applicable for JSON Converter.",,boolean,DISALLOW,TRUE,,low
76,Additional Configs,value.converter.schemas.enable,"Include schemas within each of the serialized values. Input messages must contain schema and payload fields and may not contain additional fields. For plain JSON data, set this to false. Applicable for JSON Converter.",,boolean,DISALLOW,FALSE,,low
77,Additional Configs,value.converter.value.subject.name.strategy,Determines how to construct the subject name under which the value schema is registered with Schema Registry.,,string,IGNORE,TopicNameStrategy,,low
78,Auto-restart policy,auto.restart.on.user.error,Enable connector to automatically restart on user-actionable errors.,,boolean,IGNORE,TRUE,,medium
79,Configuration File,connector.class,Identifies the connector plugin name.,Identifies the connector plugin name.  MongoDbAtlasSource streams data from MongoDB to Kafka topics.  MongoDbAtlasSink streams data from Kafka topics to MongoDB.,string,REQUIRE,,,high